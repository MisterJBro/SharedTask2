{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "Analyzing the tweet datasets, getting different kind of information and answering own questions about the data.\n",
    "For this we look into the different columns of the data. While analyzing, we want to compare the train and trial dataset, do they seem to come from the same data distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "sns.set_theme()\n",
    "%matplotlib inline\n",
    "\n",
    "train = pd.read_csv('../datasets/competition/train.csv')\n",
    "trial = pd.read_csv('../datasets/competition/trial.csv')\n",
    "test = pd.read_csv('../datasets/competition/test.csv', usecols=[1,2,3,4])\n",
    "\n",
    "# General info about the datasets\n",
    "print(f'Train - number of rows: {len(train)}, memory size {np.round(train.memory_usage(deep=True).sum()/1000)} KB')\n",
    "print(f'Trial - number of rows: {len(trial)}, memory size {np.round(trial.memory_usage(deep=True).sum()/1000)} KB')\n",
    "print(f'Test - number of rows: {len(test)}, memory size {np.round(test.memory_usage(deep=True).sum()/1000)} KB')\n",
    "print(f'Columns: \\n{train.dtypes}')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date\n",
    "In what time range were the post written? How many where written per day, per month, what is the time distribution for each time period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates\n",
    "train['date'] =  pd.to_datetime(train['date'], format='%Y-%m-%d %H:%M:%S+00:00')\n",
    "trial['date'] =  pd.to_datetime(trial['date'], format='%Y-%m-%d %H:%M:%S+00:00')\n",
    "test['date'] =  pd.to_datetime(test['date'], format='%Y-%m-%d %H:%M:%S+00:00')\n",
    "\n",
    "# Time range of plots\n",
    "print(f'Time range of train: {train[\"date\"].min().date().strftime(\"%d.%b %Y\")} to {train[\"date\"].max().date().strftime(\"%d.%b %Y\")}')\n",
    "print(f'Time range of trial: {trial[\"date\"].min().date().strftime(\"%d.%b %Y\")} to {trial[\"date\"].max().date().strftime(\"%d.%b %Y\")}')\n",
    "print(f'Time range of test: {test[\"date\"].min().date().strftime(\"%d.%b %Y\")} to {test[\"date\"].max().date().strftime(\"%d.%b %Y\")}')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Plot how many posts where written per year\n",
    "train_dates = sorted(map(lambda x: x.date().replace(month=1, day=1), train['date'].tolist()))\n",
    "trial_dates = sorted(map(lambda x: x.date().replace(month=1, day=1), trial['date'].tolist()))\n",
    "test_dates = sorted(map(lambda x: x.date().replace(month=1, day=1), test['date'].tolist()))\n",
    "X_train,Y_train = np.unique(train_dates, return_counts=True)\n",
    "X_trial,Y_trial = np.unique(trial_dates, return_counts=True)\n",
    "X_test,Y_test = np.unique(test_dates, return_counts=True)\n",
    "\n",
    "indices = np.argpartition(Y_train, -3)[-3:]\n",
    "months = list(map(lambda x: x.strftime(\"%Y\"), sorted(X_train[indices])))\n",
    "percentage = np.round(100* (np.sum(Y_train[indices]) / 500))\n",
    "\n",
    "print(f'The three years with the most tweets are: {months} having {percentage} % of all tweets')\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.title('Number of tweets written each year')\n",
    "plt.plot(X_train,Y_train, label='train', linewidth=1)\n",
    "plt.plot(X_trial,Y_trial, label='trial', linewidth=1, linestyle='dashed')\n",
    "plt.plot(X_test,Y_test, label='test', linewidth=1, linestyle='dashed')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.legend()\n",
    "\n",
    "# Number of tweets per month\n",
    "train_dates = sorted(map(lambda x: x.date().replace(day=1), train['date'].tolist()))\n",
    "trial_dates = sorted(map(lambda x: x.date().replace(day=1), trial['date'].tolist()))\n",
    "test_dates = sorted(map(lambda x: x.date().replace(day=1), test['date'].tolist()))\n",
    "X_train,Y_train = np.unique(train_dates, return_counts=True)\n",
    "X_trial,Y_trial = np.unique(trial_dates, return_counts=True)\n",
    "X_test,Y_test = np.unique(test_dates, return_counts=True)\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.title('Number of tweets written each month')\n",
    "plt.plot(X_train,Y_train, label='train', linewidth=1)\n",
    "plt.plot(X_trial,Y_trial, label='trial', linewidth=1, linestyle='dashed')\n",
    "plt.plot(X_test,Y_test, label='test', linewidth=1, linestyle='dashed')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.legend()\n",
    "\n",
    "indices = np.argpartition(Y_train, -4)[-4:]\n",
    "months = list(map(lambda x: x.strftime(\"%b %Y\"), sorted(X_train[indices])))\n",
    "percentage = np.round(100* (np.sum(Y_train[indices]) / 500))\n",
    "\n",
    "print(f'The four months with the most tweets are: {months} having {percentage} % of all tweets')\n",
    "\n",
    "# Plot how many posts where written in a day\n",
    "train_dates = sorted(map(lambda x: x.date(), train['date'].tolist()))\n",
    "trial_dates = sorted(map(lambda x: x.date(), trial['date'].tolist()))\n",
    "test_dates = sorted(map(lambda x: x.date(), test['date'].tolist()))\n",
    "X_train,Y_train = np.unique(train_dates, return_counts=True)\n",
    "X_trial,Y_trial = np.unique(trial_dates, return_counts=True)\n",
    "X_test,Y_test = np.unique(test_dates, return_counts=True)\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.title('Number of tweets written each day')\n",
    "plt.plot(X_train,Y_train, label='train', linewidth=1)\n",
    "plt.plot(X_trial,Y_trial, label='trial', linewidth=1, linestyle='dashed')\n",
    "plt.plot(X_test,Y_test, label='test', linewidth=1, linestyle='dashed')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.legend()\n",
    "\n",
    "indices = np.argpartition(Y_train, -5)[-5:]\n",
    "months = list(map(lambda x: x.strftime(\"%d.%b %Y\"), sorted(X_train[indices])))\n",
    "percentage = np.round(100* (np.sum(Y_train[indices]) / 500))\n",
    "\n",
    "print(f'The five days with the most tweets are: {months} having {percentage} % of all tweets')\n",
    "\n",
    "# Plot how many posts where written at the exact time\n",
    "train_dates = sorted(map(lambda x: x, train['date'].tolist()))\n",
    "trial_dates = sorted(map(lambda x: x, trial['date'].tolist()))\n",
    "test_dates = sorted(map(lambda x: x, test['date'].tolist()))\n",
    "X_train,Y_train = np.unique(train_dates, return_counts=True)\n",
    "X_trial,Y_trial = np.unique(trial_dates, return_counts=True)\n",
    "X_test,Y_test = np.unique(test_dates, return_counts=True)\n",
    "\n",
    "indices = np.argwhere(Y_train > 1).reshape(-1)\n",
    "months = list(map(lambda x: x.strftime(\"%H:%M:%S %d.%b %Y\"), sorted(X_train[indices])))\n",
    "indices_trial = np.argwhere(Y_trial > 1).reshape(-1)\n",
    "months_trial = list(map(lambda x: x.strftime(\"%H:%M:%S %d.%b %Y\"), sorted(X_trial[indices_trial])))\n",
    "\n",
    "print(f'Times, when atleast two tweets where written at the exact same moment: {months}')\n",
    "print('Content of some tweets written at the exact same time:')\n",
    "for x in months[:3]:\n",
    "    print(list(map(lambda x: x[:70] + '..' ,train[train['date'] == datetime.strptime(x, \"%H:%M:%S %d.%b %Y\")]['content'].tolist())))\n",
    "print(f'Number of exact time tweets in train: {len(np.argwhere(Y_train > 1))} and in trial: {len(np.argwhere(Y_trial > 1))}')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.title('Number of tweets written at the exact time point')\n",
    "plt.plot(X_train,Y_train, label='train', linewidth=1)\n",
    "plt.plot(X_trial,Y_trial, label='trial', linewidth=1, linestyle='dashed')\n",
    "plt.plot(X_test,Y_test, label='test', linewidth=1, linestyle='dashed')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets per label in train\n",
    "pos_dates = sorted(map(lambda x: x.date().replace(day=1), train[train['label']==0]['date'].tolist()))\n",
    "neg_dates = sorted(map(lambda x: x.date().replace(day=1), train[train['label']==1]['date'].tolist()))\n",
    "neu_dates = sorted(map(lambda x: x.date().replace(day=1), train[train['label']==2]['date'].tolist()))\n",
    "X_pos,Y_pos = np.unique(pos_dates, return_counts=True)\n",
    "X_neu,Y_neu = np.unique(neu_dates, return_counts=True)\n",
    "X_neg,Y_neg = np.unique(neg_dates, return_counts=True)\n",
    "\n",
    "plt.title('Number of tweets written each month')\n",
    "#plt.plot(X_neu,Y_neu, label='Other', linewidth=1)\n",
    "plt.plot(X_pos,Y_pos, label='Solidary', linewidth=1)\n",
    "plt.plot(X_neg,Y_neg, label='Anti-solidary', linewidth=1)\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "Even through the dates of the tweets alone, we can see that the tweets are likely written in the same time periods and have the same time distribution. So having the same data distribution seems also likely from this fact alone.\n",
    "\n",
    "As can be seen most posts are written in 2021 and also each year the number is descending having less tweets, except the year 2018, where we also have nearly as much posts as 2021. However, the tweets are not evenly collected during the years, with some months, especially 4 ones, having clearly the most ones with nearly 50% of the tweets. But even within these months, the number of tweets is not evenly during the month. Dec 2018 and Nov 2021 have a somewhat more equal distribution, while Feb 2020 and Aug 2021 clearly have one maxima day, respectively.\n",
    "\n",
    "What happened at these days? Looking at these days, I did not find anything related to islam solidarity, therefore, I consider these days to be the result of the data crawling process, unintentionally these days were preferred. Lastly we are looking at tweets that are written at the exact time. We find only very few occurrences of this happening, most of the time the datetimes are unique. Although it is not unlikely on twitter to find two tweets written at the exact same time (even at the same second!), it is very unlikely to find such tweets in a dataset designed on a very specific topic with only 500 samples available. That's why I assumed that these tweets are simply the same ones, and looking at the content we see that this is true. We count 11 and 12 of these copied tweets in train and trial by just using the time. Nevertheless, sometimes there are small changes in the content e.g. removal of a emoji in one tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country\n",
    "Next we investigate the country column of the tweets. Where do the tweets come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_country = train.groupby('country').count().sort_values(by='date', ascending=False).drop(columns=['content', 'label', 'lang']).rename(columns={'date': 'train'})\n",
    "trial_country = trial.groupby('country').count().sort_values(by='date', ascending=False).drop(columns=['content', 'lang']).rename(columns={'date': 'trial'})\n",
    "test_country = test.groupby('country').count().sort_values(by='date', ascending=False).drop(columns=['content', 'lang']).rename(columns={'date': 'test'})\n",
    "print(f'Top 3 countries UK US and Germany make {train_country[\"train\"][:3].sum() / 5}% of all tweets')\n",
    "pd.concat([train_country, trial_country, test_country], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lang = train.groupby('lang').count().sort_values(by='date', ascending=False).drop(columns=['content', 'label', 'country']).rename(columns={'date': 'train'})\n",
    "trial_lang = trial.groupby('lang').count().sort_values(by='date', ascending=False).drop(columns=['content', 'country']).rename(columns={'date': 'trial'})\n",
    "test_lang = test.groupby('lang').count().sort_values(by='date', ascending=False).drop(columns=['content', 'country']).rename(columns={'date': 'test'})\n",
    "print(f'Tweets with lang \"in\": {train[train.lang == \"in\"][\"content\"].tolist()}, {trial[trial.lang == \"in\"][\"content\"].tolist()}')\n",
    "pd.concat([train_lang, trial_lang, test_lang], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pre = pd.read_csv('../datasets/train_pre.csv')\n",
    "#train_pre_lens = train_pre['content'].apply(lambda x: len(str(x))).tolist()\n",
    "\n",
    "train_lens = train['content'].apply(lambda x: len(x)).tolist()\n",
    "print(f'Longest post in train with {max(train_lens)} symbols: {train[\"content\"].tolist()[np.argmax(train_lens)]}')\n",
    "trial_lens = trial['content'].apply(lambda x: len(x)).tolist()\n",
    "test_lens = test['content'].apply(lambda x: len(x)).tolist()\n",
    "\n",
    "plt.title('Length of tweets (number of characters)')\n",
    "#plt.hist(train_pre_lens, 20, label='train', linewidth=1)\n",
    "plt.hist(train_lens, 20, label='train', linewidth=1)\n",
    "plt.hist(trial_lens, 20, label='trial', linewidth=1, alpha=0.5)\n",
    "plt.hist(test_lens, 20, label='trial', linewidth=1, alpha=0.5)\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the hateful posts\n",
    "train_pre = pd.read_csv('../datasets/preprocessed/train.csv')\n",
    "t = train[train.label == 1]['content'].tolist()\n",
    "tp = train_pre[train_pre.label == 1]['content'].tolist()\n",
    "for a,b in zip(t, tp):\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the solidary posts\n",
    "train_pre = pd.read_csv('../datasets/train_pre.csv')\n",
    "t = train[train.label == 0]['content'].tolist()\n",
    "tp = train_pre[train_pre.label == 0]['content'].tolist()\n",
    "for a,b in zip(t, tp):\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train.label == 2]['content'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "- Longest post has 824 symbols! but mostly users are tagged, which is not useful information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train.groupby('label').count().sort_values(by='date', ascending=False).drop(columns=['content', 'lang', 'country']).rename(columns={'date': 'train'})\n",
    "train_label.index = train_label.index.map(lambda x: {2: 'Other', 0: 'Solidarity', 1: 'Anti-Solidarity'}[x])\n",
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n",
    "Helpful scripts for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missclassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../datasets/preprocessed/dev.csv')\n",
    "pred = np.loadtxt('labels.txt')\n",
    "df['pred'] = pred\n",
    "df['pred'] = df['pred'].astype('int32')\n",
    "df['label'] = df['label'].astype('int32')\n",
    "df[df.label != df.pred].to_csv('test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking old submissions\n",
    "import numpy as np\n",
    "a = np.loadtxt('../old_submissions/labels0.84.txt')\n",
    "b = np.loadtxt('../old_submissions/labels0.848.txt')\n",
    "c = np.loadtxt('../old_submissions/labels0.856.txt')\n",
    "d = np.loadtxt('../old_submissions/labels.txt')\n",
    "\n",
    "print(f'Labels 0: {(a == 0).sum()},  1: {(a == 1).sum()},  2: {(a == 2).sum()}')\n",
    "print(f'Labels 0: {(b == 0).sum()},  1: {(b == 1).sum()},  2: {(b == 2).sum()}')\n",
    "print(f'Labels 0: {(c == 0).sum()},  1: {(c == 1).sum()},  2: {(c == 2).sum()}')\n",
    "print(f'Labels 0: {(d == 0).sum()},  1: {(d == 1).sum()},  2: {(d == 2).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f86112d799df4baee5103ec91ce2afe3a359a0aa80ab8e0ad83840cf07e57e31"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('poetryT5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
