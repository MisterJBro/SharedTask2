{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Allowed Words\n",
    "To make the other dataset more similar to the train.csv + trial.csv, we filter the most common words out of them and build a whitelist. Only these words are allowed, other words are filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "train = pd.read_csv('../datasets/train_pre.csv', usecols=[1])\n",
    "trial = pd.read_csv('../datasets/trial_pre.csv', usecols=[1])\n",
    "all = pd.concat([train, trial]).reset_index(drop=True)\n",
    "all = ' . '.join([str(x) for x in all['content'].tolist()]).lower()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Lemmatize words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "all = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(all)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(all)\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "print(f'Total number of words: {len(fdist.items())}')\n",
    "print(f'20 Most common words: {fdist.most_common(20)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that are used atleast twice\n",
    "more = [w for w, i in fdist.items() if i > 4]\n",
    "more = pd.Series(more)\n",
    "more.to_csv('../datasets/whitelist/whitelist1.csv', index=False, header=False)\n",
    "\n",
    "# Words that are only used once\n",
    "once = [w for w,i in fdist.items() if i <= 4]\n",
    "once = pd.Series(once)\n",
    "once.to_csv('../datasets/whitelist/whitelist2.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create whitelist\n",
    "white1 = pd.read_csv('../datasets/whitelist/whitelist1.csv', names=['content'])\n",
    "white2 = pd.read_csv('../datasets/whitelist/whitelist2.csv', names=['content'])\n",
    "white = pd.concat([white1, white2]).reset_index(drop=True)\n",
    "white = white.sample(frac=1).reset_index(drop=True)\n",
    "white = list(set(white['content'].tolist()))\n",
    "white = pd.DataFrame(white)\n",
    "white.to_csv('../datasets/whitelist/whitelist.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f86112d799df4baee5103ec91ce2afe3a359a0aa80ab8e0ad83840cf07e57e31"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('poetryT5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
